{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbdd5847",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import lime\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d0e23",
   "metadata": {},
   "source": [
    "# API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e600f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads csv file\n",
    "'''\n",
    "df = the data file path\n",
    "sep = separator for dividing each column in the csv file\n",
    "'''\n",
    "def read_csv(df, sep = \",\"):\n",
    "    return pd.read_csv(df, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01c1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the dataframe to csv file and save \n",
    "'''\n",
    "df = the dataframe to convert\n",
    "filepath = the filepath which will save the csv file\n",
    "'''\n",
    "def to_csv(df, filepath):\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f7ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns list of columns in the dataframe\n",
    "'''\n",
    "df = the dataframe to return columns it has\n",
    "'''\n",
    "def return_columns(df):\n",
    "    return df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aec3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns list of classes the dataset has\n",
    "'''\n",
    "target_column = the array of targets or the column which has classes in the dataframe\n",
    "'''\n",
    "def return_list_of_class(target_column):\n",
    "    return list(np.unique(target_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9ed4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appends two dataframes\n",
    "'''\n",
    "df1, df2 = the dataframes to merge\n",
    "'''\n",
    "def merge_datasets(df1, df2):\n",
    "    return pd.DataFrame(df1.append(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e209573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates decision tree classifier\n",
    "def create_decision_tree(criterion=\"gini\", splitter=\"best\"):\n",
    "    model = DecisionTreeClassifier(criterion=criterion, splitter=splitter, random_state=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8f3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(fit)s decision tree\n",
    "'''\n",
    "train_X = the dataframe of training data but for features\n",
    "train_y = the array of training data of targets\n",
    "model = the model to train\n",
    "'''\n",
    "def fit_decision_tree(train_X, train_y, model):\n",
    "    model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c74c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads(reads) saved ML model in binary mode\n",
    "'''\n",
    "file_to_open = the file to open to load ML models\n",
    "'''\n",
    "def load_ML_model(file_to_open):\n",
    "    filehandler = open(file_to_open, 'rb') \n",
    "    return pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa22f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves ML model in binary mode\n",
    "'''\n",
    "file_name = the name of the file without any extensions\n",
    "model = the model to save\n",
    "'''\n",
    "def save_ML_model(file_name, model):\n",
    "    filehandler = open(file_name + '.obj', 'wb') \n",
    "    pickle.dump(model, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e42b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the accuracy of the model\n",
    "'''\n",
    "testcases = the array of testing data used for evaluation\n",
    "prediction_result = the array of prediction results made against testcases\n",
    "'''\n",
    "def calculate_accuracy(testcases, prediction_result):\n",
    "    return round(accuracy_score(testcases, prediction_result) * 100.00, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1237ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains a decision tree classifier based on agreed and disagreed data\n",
    "'''\n",
    "all_cases = the dataframe of all datapoints which two classifiers agreed and disagreed\n",
    "features = the features of data\n",
    "'''\n",
    "def train_decision_tree_agreed_disagreed(all_cases, features):\n",
    "    train_X = all_cases[features]\n",
    "    train_y = all_cases.Agreed\n",
    "    model = create_decision_tree()\n",
    "    fit_decision_tree(train_X, train_y, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab96f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate smodel against the data\n",
    "'''\n",
    "model = the model to use for evaluation/testing\n",
    "testcases = the dataframe of testing data for making predictions\n",
    "'''\n",
    "def generate_predictions(model, testcases):\n",
    "    return model.predict(testcases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5dcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the similarity between two models\n",
    "'''\n",
    "prediction_result1 = the array of prediction outputs made by classifier 1\n",
    "prediction_result2 = the array of prediction outputs made by classifier 2\n",
    "'''\n",
    "def calculate_similarity(prediction_result1, prediction_result2):\n",
    "    return round(accuracy_score(prediction_result1, prediction_result2) * 100.00, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e4baace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the number of disagreed data points\n",
    "'''\n",
    "testcases = the array of testing data used for evaluating model\n",
    "similarity = the similarity between two classifiers\n",
    "'''\n",
    "def number_of_disagreed_cases(testcases, similarity):\n",
    "    wrong_proportion = 1 - (similarity / 100)\n",
    "    return round(len(testcases) * wrong_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8515b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns set of datapoints both models agreed on\n",
    "'''\n",
    "testcases = the array of testing data used for evaluating model\n",
    "prediction_result1 = the array of prediction outputs made by classifier 1\n",
    "prediction_result2 = the array of prediction outputs made by classifier 2\n",
    "'''\n",
    "def return_agreed_cases(testcases, prediction_result1, prediction_result2):\n",
    "    filters = []\n",
    "    for i in range(len(testcases)):\n",
    "        filters.append(prediction_result1[i] == prediction_result2[i])\n",
    "    agreed = testcases[filters]\n",
    "    agreed.loc[:,'Agreed'] = 1\n",
    "    return agreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0530569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns set of datapoints the models disagreed on\n",
    "'''\n",
    "testcases = the array of testing data used for evaluating model\n",
    "prediction_result1 = the array of prediction outputs made by classifier 1\n",
    "prediction_result2 = the array of prediction outputs made by classifier 2\n",
    "'''\n",
    "def return_disagreed_cases(testcases, prediction_result1, prediction_result2):\n",
    "    filters = return_agreed_cases(testcases,prediction_result1, prediction_result2)\n",
    "    # filters out 'agreed' data\n",
    "    disagreed = testcases.loc[testcases.index.difference(filters.index)]\n",
    "    disagreed.loc[:,'Agreed'] = 0\n",
    "    return disagreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3c9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs the feature importance of tree-based models\n",
    "'''\n",
    "fittedmodel = the trained tree-based model\n",
    "features = the list of features of the fittedmodel\n",
    "'''\n",
    "def print_feature_importance(fittedmodel, features):\n",
    "    feature_importance = fittedmodel.feature_importances_\n",
    "    feature_importance_output = pd.DataFrame(feature_importance, features)\n",
    "    feature_importance_output.set_axis(['Output'], axis=1, inplace=True)\n",
    "    print(feature_importance_output)\n",
    "    ax = feature_importance_output.plot.barh(figsize=(9,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42e8a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs the feature importance of linear models\n",
    "'''\n",
    "fittedmodel = the trained linear model\n",
    "features = the list of features of the fittedmodel\n",
    "'''\n",
    "def print_feature_importance_regression(fittedmodel, features):\n",
    "    importance = fittedmodel.coef_\n",
    "    importance = importance.reshape(-1,1)\n",
    "    df = pd.DataFrame(importance, features).round()\n",
    "    df.set_axis(['Feature coefficient'], axis=1, inplace=True)\n",
    "    print(df)\n",
    "    ax = df.plot.barh(figsize=(9,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12cb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts feature importance of tree-based models together to compare and outputs a horizontal bar plot\n",
    "'''\n",
    "model1, model2 = the tree-based ML models to compare\n",
    "features = the list of features of the models\n",
    "name1, name2 = the name of each model to denote on the bar plot\n",
    "'''\n",
    "def feature_importance_comparison(model1, model2, features, name1, name2):\n",
    "    feature_importance1 = model1.feature_importances_\n",
    "    feature_importance2 = model2.feature_importances_\n",
    "    feature_importance1_output = pd.DataFrame(feature_importance1, features)\n",
    "    feature_importance2_output = pd.DataFrame(feature_importance2, features)\n",
    "    concat_df = pd.concat([feature_importance1_output, feature_importance2_output],axis=1)\n",
    "    concat_df.set_axis([name1,name2], axis=1, inplace=True)\n",
    "    ax=concat_df.plot.barh(figsize=(9,7))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "015de23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts feature importance of linear models together to compare and outputs a horizontal bar plot\n",
    "'''\n",
    "model1, model2 = the linear ML models to compare\n",
    "features = the list of features of the models\n",
    "name1, name2 = the name of each model to denote on the bar plot\n",
    "'''\n",
    "def feature_importance_comparison_regression(model1, model2, features, name1, name2):\n",
    "    feature_importance1 = (model1.coef_).reshape(-1,1)\n",
    "    feature_importance2 = (model2.coef_).reshape(-1,1)\n",
    "    feature_importance1_output = pd.DataFrame(feature_importance1, features)\n",
    "    feature_importance2_output = pd.DataFrame(feature_importance2, features)\n",
    "    concat_df = pd.concat([feature_importance1_output, feature_importance2_output],axis=1)\n",
    "    concat_df.set_axis([name1,name2], axis=1, inplace=True)\n",
    "    ax=concat_df.plot.barh(figsize=(9,7))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78422e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs feature importance of model trained on disagreed data points\n",
    "'''\n",
    "features = the list of features of the models\n",
    "agreed_cases = the dataframe of data points both classifier agreed on\n",
    "disagreed_cases = the dataframe of data points the models disagreed on\n",
    "'''\n",
    "def feature_importance_for_disagreement(features, agreed_cases, disagreed_cases):\n",
    "    all_cases = pd.DataFrame(agreed_cases.append(disagreed_cases))\n",
    "    X_train = all_cases[features]\n",
    "    y_train = all_cases['Agreed']\n",
    "    model = create_decision_tree()\n",
    "    fit_decision_tree(X_train, y_train, model)\n",
    "    print_feature_importance(model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d7d29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs LIME for disagreement between two binary classifiers\n",
    "'''\n",
    "train1, train2 = the dataframes used for training model1 and model2\n",
    "all_disagreed_testcases = the dataframe of all datapoints which classifiers disagreed on\n",
    "model1, model2 = the models to compare\n",
    "features = the list of features of the models\n",
    "classes = the classes of the models\n",
    "index = the index of instance to examine\n",
    "'''\n",
    "def disagreed_LIME(train1, train2, all_disagreed_testcases, model1, model2, features, classes, index = None):\n",
    "    train1_numpy = train1.to_numpy()\n",
    "    train2_numpy = train2.to_numpy()\n",
    "    all_disagreed_testcases_numpy = all_disagreed_testcases[features].to_numpy()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train1_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes)\n",
    "\n",
    "    # if sample not selected, randomly select one\n",
    "    if index == None:\n",
    "        index = np.random.randint(0, len(all_disagreed_testcases_numpy))\n",
    "    disagreed_case = all_disagreed_testcases_numpy[index]\n",
    "\n",
    "    exp1 = explainer.explain_instance(disagreed_case, model1.predict_proba, num_features=len(features))\n",
    "    exp1_map = exp1.as_map()\n",
    "    exp1.show_in_notebook()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train2_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    exp2 = explainer.explain_instance(disagreed_case, model2.predict_proba, num_features=len(features))\n",
    "    exp2_map = exp2.as_map()\n",
    "    exp2.show_in_notebook()\n",
    "    \n",
    "    # the class the model predicted\n",
    "    index1= model1.predict(disagreed_case.reshape(1,-1))[0]\n",
    "    index2= model2.predict(disagreed_case.reshape(1,-1))[0]\n",
    "    \n",
    "    # use the class index to select the final results\n",
    "    exp1_result = classes[index1]\n",
    "    exp2_result = classes[index2]\n",
    "    \n",
    "    return exp1_map[1], exp2_map[1], disagreed_case, exp1_result, exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a21e5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs LIME for disagreement between two multiclass classifiers\n",
    "'''\n",
    "train1, train2 = the dataframes used for training model1 and model2\n",
    "all_disagreed_testcases = the dataframe of all datapoints which classifiers disagreed on\n",
    "model1, model2 = the models to compare\n",
    "features = the list of features of the models\n",
    "classes = the classes of the models\n",
    "index = the index of instance to examine\n",
    "'''\n",
    "def disagreed_LIME_multi(train1, train2, all_disagreed_testcases, model1, model2, features, classes, index = None):\n",
    "    train1_numpy = train1.to_numpy()\n",
    "    train2_numpy = train2.to_numpy()\n",
    "    all_disagreed_testcases_numpy = all_disagreed_testcases[features].to_numpy()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train1_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes)\n",
    "\n",
    "    if index == None:\n",
    "        index = np.random.randint(0, len(all_disagreed_testcases_numpy))\n",
    "    disagreed_case = all_disagreed_testcases_numpy[index]\n",
    "    \n",
    "    # substract min from each index as the index of class should start from 0\n",
    "    index1= int(model1.predict(disagreed_case.reshape(1,-1))[0]) - min(classes)\n",
    "    index2= int(model2.predict(disagreed_case.reshape(1,-1))[0]) - min(classes)\n",
    "\n",
    "    exp1 = explainer.explain_instance(disagreed_case, model1.predict_proba, num_features=len(features), labels=[index1])\n",
    "    exp1_map = exp1.as_map()\n",
    "    exp1.show_in_notebook()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train2_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    exp2 = explainer.explain_instance(disagreed_case, model2.predict_proba, num_features=len(features), labels=[index2])\n",
    "    exp2_map = exp2.as_map()\n",
    "    exp2.show_in_notebook()\n",
    "    \n",
    "    exp1_result = classes[index1]\n",
    "    exp2_result = classes[index2]\n",
    "    \n",
    "    return exp1_map[index1], exp2_map[index2], disagreed_case, exp1_result, exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20f467c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs LIME for regression\n",
    "'''\n",
    "train_set = the dataframes used for training regression model\n",
    "test_set = the dataframe for evaluating the performance of regressor\n",
    "model = the regression model to utilise\n",
    "features = the list of features of the model\n",
    "class_name = the name of the output column\n",
    "index = the index of instance to examine\n",
    "'''\n",
    "def regression_LIME(train_set, test_set, model, features, class_name, index = None):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train_set.to_numpy(),\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=class_name,\n",
    "                                                       verbose=True,\n",
    "                                                       mode='regression')\n",
    "    test_set_numpy = test_set.to_numpy()\n",
    "    if index == None:\n",
    "        index = np.random.randint(0, len(test_set_numpy))\n",
    "    exp = explainer.explain_instance(test_set_numpy[index], model.predict, num_features=len(features))\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3715a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts LIME bar plots together\n",
    "'''\n",
    "features = the list of features of the models\n",
    "exp1_map, exp2_map = the dictionary type of explanations\n",
    "exp1_result, exp2_result = the predicted classes\n",
    "'''\n",
    "def LIME_compare_bar_plot(features, exp1_map, exp2_map, exp1_result, exp2_result):\n",
    "    # sort outputs in alphabetical order of feature names\n",
    "    exp1_map.sort()\n",
    "    exp2_map.sort()\n",
    "    \n",
    "    # convert dictionary type output to dataframe for plotting\n",
    "    exp1_map_df = pd.DataFrame(exp1_map)\n",
    "    exp2_map_df = pd.DataFrame(exp2_map)\n",
    "    \n",
    "    # append column of feature names\n",
    "    exp1_map_df[0] = features\n",
    "    exp2_map_df[0] = features\n",
    "    \n",
    "    # merge outputs into one to plot\n",
    "    merged_df= pd.merge(exp1_map_df, exp2_map_df, on=0)\n",
    "    \n",
    "    ax = merged_df.plot.barh(figsize=(9,7))\n",
    "    plt.yticks(np.arange(len(features)),features)\n",
    "    plt.legend([exp1_result, exp2_result])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfb89909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draws sankey diagram of two classifiers\n",
    "'''\n",
    "title = the title of the sankey diagram \n",
    "classes = the list of classes which classifiers can predict\n",
    "pred1, pred2 = the prediction results made by classifier 1 and classifier 2, respectively\n",
    "ground_truth = the actual values\n",
    "'''\n",
    "def draw_sankey_diagram(title, classes, pred1, pred2, ground_truth):\n",
    "    number_of_classes = len(classes)\n",
    "\n",
    "    # generate labels\n",
    "    classes_labels = list()\n",
    "    for i in range(max(classes), min(classes) - 1, -1):\n",
    "        classes_labels.append(\"Class \" + str(i))\n",
    "    \n",
    "    labels = classes_labels * 3\n",
    "    \n",
    "    # generate confusion matrix for each model\n",
    "    confusion_matrix1 = confusion_matrix(ground_truth, pred1, labels=classes)\n",
    "    confusion_matrix2 = confusion_matrix(ground_truth, pred2, labels=classes)\n",
    "    \n",
    "    targets = list()\n",
    "    for i in range(number_of_classes * 2 - 1, number_of_classes * 2 - number_of_classes - 1, -1):\n",
    "        for j in range(1, 1 + number_of_classes):\n",
    "            targets.append(i)\n",
    "    \n",
    "    start = targets.copy()\n",
    "    \n",
    "    for i in range(number_of_classes):\n",
    "        for j in range(3 * number_of_classes - 1, 2 * number_of_classes -1, -1):\n",
    "            targets.append(j)\n",
    "\n",
    "    sources = list()\n",
    "    for i in range(number_of_classes):\n",
    "        for j in range(number_of_classes - 1, -1, -1):\n",
    "            sources.append(j)\n",
    "\n",
    "    sources += start\n",
    "    \n",
    "    # generate values (puts the values from each confusion matrix together)\n",
    "    values = list(confusion_matrix1.ravel()) + list(confusion_matrix2.ravel())\n",
    "    \n",
    "    my_colors = [('rgba('+str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))) for i in range(len(classes_labels))]\n",
    "    my_colors_node = []\n",
    "    my_colors_opac = []\n",
    "\n",
    "    for rgba in my_colors:\n",
    "        my_colors_node.append(rgba + ',0.8)')\n",
    "        my_colors_opac.append(rgba + ',0.4)')\n",
    "    my_colors_opac = my_colors_opac * number_of_classes\n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),    \n",
    "        label = labels,\n",
    "        color = my_colors_node * 3\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = sources,\n",
    "        target = targets,\n",
    "        value = values,\n",
    "        color = my_colors_opac[::-1] + my_colors_opac[::-1]\n",
    "    ))])\n",
    "\n",
    "    fig.update_layout(title_text=title, font_size=12)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f8f1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates linear SHAP explainer\n",
    "'''\n",
    "model = the model to explain\n",
    "train_data = the training data used for training the model\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def create_LinearExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.LinearExplainer(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26b2e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates Kernel SHAP explainer for classification\n",
    "'''\n",
    "model = the model to explain\n",
    "train_data = the training data used for training the model\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def create_Classification_KernelExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.KernelExplainer(model.predict_proba, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1310d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates Kernel SHAP explainer for regression\n",
    "'''\n",
    "model = the model to explain\n",
    "train_data = the training data used for training the model\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def create_Regression_KernelExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.KernelExplainer(model.predict, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cf39c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates tree SHAP explainer\n",
    "'''\n",
    "model = the model to explain\n",
    "train_data = the training data used for training the model\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def create_TreeExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.TreeExplainer(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7da60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples data for SHAP\n",
    "'''\n",
    "data = the input dataset\n",
    "features = the list of names of the features in the dataset\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def SHAP_sample_data(data, features, sample = None):\n",
    "    if sample == None:\n",
    "        sample = len(data) / 5\n",
    "    return shap.sample(data[features], sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f7e4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the information of SHAP such as SHAP values and the prediction without visualisations\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "model = the model to explain\n",
    "test_data = the dataframe of all testing data\n",
    "index = the index of data to examine\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def print_SHAP_info(shap_explainer, model, test_data, index, sample = None):\n",
    "    \n",
    "    if sample != None:\n",
    "        test_data = shap.sample(test_data, sample)    \n",
    "    \n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "    \n",
    "    # checks if the list of SHAP values is wrapped in extra list\n",
    "    if len(shap_values) == 1 and len(test_data) != 1:\n",
    "        shap_values = shap_values[0]\n",
    "\n",
    "    print(\"Base Value : \", shap_explainer.expected_value)\n",
    "    print()\n",
    "    print(\"Shap Values for Sample %d: \" %(index), shap_values[index])\n",
    "    print(\"\\n\")\n",
    "    print(\"Prediction From Model                            : \", model.predict((test_data.iloc[index]).to_numpy().reshape(1,-1))[0])\n",
    "    print(\"Input instance                                  : \", test_data.iloc[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53c35fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs feature importance of a model using summary plot\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "train_data = the dataframe used for training the model\n",
    "features = the list of names of the features in the data\n",
    "classes = the list of names of the classes in the data\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def shap_feature_importance(shap_explainer, train_data, features, classes, sample = None):\n",
    "    shap.initjs()\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    shap_values = shap_explainer.shap_values(train_data)\n",
    "    return shap.summary_plot(shap_values, train_data, feature_names=features, plot_type=\"bar\", class_names=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d286bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs global explanation using force plot for classification problems\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "test_data = the dataframe for evaluating the performance of the model\n",
    "class_index = the index of class to look into\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "\n",
    "def global_classification_force_plot(shap_explainer, test_data, class_index, sample=None):\n",
    "    shap.initjs()\n",
    "    \n",
    "    if sample != None:\n",
    "        test_data = shap.sample(test_data, sample)\n",
    "    \n",
    "    base_value = shap_explainer.expected_value\n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "    \n",
    "    return shap.force_plot(base_value[class_index], shap_values[class_index], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "189da472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output global explanation of model using force plot for regression problems\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "test_data = the dataframe for evaluating the performance of the model\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def global_regression_force_plot(shap_explainer, test_data, sample = None):\n",
    "    shap.initjs()\n",
    "\n",
    "    base_value = shap_explainer.expected_value\n",
    "    \n",
    "    if sample != None:\n",
    "        test_data = shap.sample(test_data, sample)\n",
    "    \n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "    \n",
    "    # checks if the list of SHAP values is not wrapped in extra list\n",
    "    # if it is, return the first element - shap_values[0]\n",
    "    if(len(shap_values) == len(test_data)):\n",
    "        return shap.force_plot(base_value, shap_values, test_data)\n",
    "    return shap.force_plot(base_value, shap_values[0], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc91a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs local explanation using waterfall plot for classification problems\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "data = the data for evaluating the performance of the model\n",
    "class_index = the index of class to look into\n",
    "sample_index = the index of sample to examine\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def classification_waterfall_plot(shap_explainer, data, class_index, sample_index = None, sample = None):\n",
    "    shap.initjs()\n",
    "    \n",
    "    if sample != None:\n",
    "        data = shap.sample(data, sample)\n",
    "\n",
    "    shap_values = shap_explainer.shap_values(data)\n",
    "    \n",
    "    if sample_index == None:\n",
    "        sample_index = np.random.randint(0, len(data))\n",
    "    \n",
    "    return shap.waterfall_plot(shap.Explanation(values=shap_values[class_index][sample_index], \n",
    "                                     base_values=shap_explainer.expected_value[class_index],\n",
    "                                     data=data.iloc[sample_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53f31baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs local explanation using waterfall plot for regression problems\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "test_data = the dataframe for evaluating the performance of the model\n",
    "sample_index = the index of sample to examine\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def regression_waterfall_plot(shap_explainer, test_data, sample_index = None, sample = None):\n",
    "    shap.initjs()\n",
    "    \n",
    "    if sample != None:\n",
    "        test_data = shap.sample(test_data, sample)\n",
    "\n",
    "    shap_values = shap_explainer(test_data)\n",
    "    \n",
    "    if sample_index == None:\n",
    "        sample_index = np.random.randint(0, len(test_data))\n",
    "    \n",
    "    return shap.plots.waterfall(shap_values[sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ea36c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs local explanation using waterfall plot for any problems which used KernelExplainer\n",
    "'''\n",
    "shap_explainer = the SHAP explainer for explaining instances\n",
    "test_data = the dataframe for evaluating the performance of the model\n",
    "sample_index = the index of sample to examine\n",
    "sample = the number of samples to select\n",
    "'''\n",
    "def kernel_waterfall_plot(shap_explainer, test_data, sample_index = None, sample = None):\n",
    "    shap.initjs()\n",
    "    \n",
    "    if sample != None:\n",
    "        test_data = shap.sample(test_data, sample)\n",
    "    \n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "    \n",
    "    if sample_index == None:\n",
    "        sample_index = np.random.randint(0, len(test_data))\n",
    "    \n",
    "    return shap.plots._waterfall.waterfall_legacy(shap_explainer.expected_value[0],\n",
    "                                                  shap_values[0][sample_index],\n",
    "                                                  feature_names = test_data.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
