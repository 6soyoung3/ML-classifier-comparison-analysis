{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12f17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_values(df):\n",
    "    print(df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf55620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_2_predictors(dataset1, dataset2, features):\n",
    "    predictor1 = dataset1[features]\n",
    "    predictor2 = dataset2[features]\n",
    "    return predictor1, predictor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80377370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_2_targets(dataset1, dataset2, outcome):\n",
    "    target1 = dataset1[outcome]\n",
    "    target2 = dataset2[outcome]\n",
    "    return target1, target2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89386988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_data(dataset_X, dataset_y):\n",
    "    return train_test_split(dataset_X, dataset_y, test_size=0.25, random_state=1, shuffle = True)\n",
    "#     return train_test_split(dataset_X, dataset_y, random_state=1, shuffle = True, stratify=dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e209573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_tree(criterion=\"gini\", splitter=\"best\", max_depth=6):\n",
    "    model = DecisionTreeClassifier(criterion=criterion, splitter=splitter, random_state=1, max_depth=max_depth)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8f3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decision_tree(train_X, train_y, model):\n",
    "    model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e42b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(testcases, prediction_result):\n",
    "    return round(accuracy_score(testcases, prediction_result) * 100.00, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1237ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree_agreed_disagreed(all_cases, features):\n",
    "    train_X = all_cases[features]\n",
    "    train_y = all_cases.Agreed\n",
    "    model = create_decision_tree()\n",
    "    fit_decision_tree(train_X, train_y, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d38e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_names e.g. ['0', '1']\n",
    "def visualise_decision_tree(model, features, class_names):\n",
    "    fig = plt.figure(figsize=(40,20))\n",
    "    visualisation = tree.plot_tree(model, feature_names=features, class_names=class_names, filled=True)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa22f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ML_model(file_name_for_save, model_to_save):\n",
    "    filehandler = open(file_name_for_save + '.obj', 'wb') \n",
    "    pickle.dump(model_to_save, filehandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843b746",
   "metadata": {},
   "source": [
    "# For users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7b64095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(filepath, sep=None):\n",
    "    return pd.read_csv(filepath, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab96f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, testcases):\n",
    "    return model.predict(testcases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c74c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ML_model(file_to_open):\n",
    "    filehandler = open(file_to_open, 'rb') \n",
    "    return pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5dcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(prediction_result1, prediction_result2):\n",
    "    return round(accuracy_score(prediction_result1, prediction_result2) * 100.00, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4baace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_disagreed_cases(testcases, similarity):\n",
    "    wrong_proportion = 1 - (similarity / 100)\n",
    "    return round(len(testcases) * wrong_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8515b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_agreed_cases(testcases, prediction_result1, prediction_result2):\n",
    "    filters = []\n",
    "    for i in range(len(testcases)):\n",
    "        filters.append(prediction_result1[i] == prediction_result2[i])\n",
    "    disagreed = testcases[filters]\n",
    "    disagreed.loc[:,'Agreed'] = 1\n",
    "    return disagreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0530569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_disagreed_cases(testcases, prediction_result1, prediction_result2):\n",
    "    filters = return_agreed_cases(testcases,prediction_result1, prediction_result2)\n",
    "    agreed = testcases.loc[testcases.index.difference(filters.index)]\n",
    "    agreed.loc[:,'Agreed'] = 0\n",
    "    return agreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d3c9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     return feature_importance_output.plot.barh()\n",
    "def print_feature_importance(fittedmodel, features):\n",
    "    feature_importance = fittedmodel.feature_importances_\n",
    "    feature_importance_output = pd.DataFrame(feature_importance, features)\n",
    "    feature_importance_output.set_axis(['Output'], axis=1, inplace=True)\n",
    "    print(feature_importance_output)\n",
    "    ax = feature_importance_output.plot.barh(figsize=(9,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42e8a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance_regression(fittedmodel, features):\n",
    "    importance = fittedmodel.coef_\n",
    "    importance = importance.reshape(-1,1)\n",
    "    df = pd.DataFrame(importance, features)\n",
    "    df.set_axis(['Feature coefficient'], axis=1, inplace=True)\n",
    "    ax = df.plot.barh(figsize=(9,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78422e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_for_disagreement(features, agreed_cases, disagreed_cases, model = None):\n",
    "    all_cases = pd.DataFrame(agreed_cases.append(disagreed_cases))\n",
    "    X_train = all_cases[features]\n",
    "    y_train = all_cases['Agreed']\n",
    "    if model == None:\n",
    "        model = create_decision_tree()\n",
    "        fit_decision_tree(X_train, y_train, model)\n",
    "    print_feature_importance(model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c12cb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_comparison(model1, model2, features, name1, name2):\n",
    "    feature_importance1 = model1.feature_importances_\n",
    "    feature_importance2 = model2.feature_importances_\n",
    "    feature_importance1_output = pd.DataFrame(feature_importance1, features)\n",
    "    feature_importance2_output = pd.DataFrame(feature_importance2, features)\n",
    "    concat_df = pd.concat([feature_importance1_output, feature_importance2_output],axis=1)\n",
    "    concat_df.set_axis([name1,name2], axis=1, inplace=True)\n",
    "    ax=concat_df.plot.barh(figsize=(9,7))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "015de23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_comparison_regression(model1, model2, features, name1, name2):\n",
    "    feature_importance1 = (model1.coef_).reshape(-1,1)\n",
    "    feature_importance2 = (model2.coef_).reshape(-1,1)\n",
    "    feature_importance1_output = pd.DataFrame(feature_importance1, features)\n",
    "    feature_importance2_output = pd.DataFrame(feature_importance2, features)\n",
    "    concat_df = pd.concat([feature_importance1_output, feature_importance2_output],axis=1)\n",
    "    concat_df.set_axis([name1,name2], axis=1, inplace=True)\n",
    "    ax=concat_df.plot.barh(figsize=(9,7))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d7d29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagreed_LIME(train1, train2, all_disagreed_testcases, model1, model2, features, classes):\n",
    "    train1_numpy = train1.to_numpy()\n",
    "    train2_numpy = train2.to_numpy()\n",
    "    all_disagreed_testcases_numpy = all_disagreed_testcases[features].to_numpy()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train1_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    i = np.random.randint(0, len(all_disagreed_testcases_numpy))\n",
    "    disagreed_case = all_disagreed_testcases_numpy[i]\n",
    "\n",
    "    exp1 = explainer.explain_instance(disagreed_case, model1.predict_proba, num_features=len(features))\n",
    "    exp1_map = exp1.as_map()\n",
    "    exp1.show_in_notebook()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train2_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    exp2 = explainer.explain_instance(disagreed_case, model2.predict_proba, num_features=len(features))\n",
    "    exp2_map = exp2.as_map()\n",
    "    exp2.show_in_notebook()\n",
    "    \n",
    "    index1= model1.predict(disagreed_case.reshape(1,-1))[0]\n",
    "    index2= model2.predict(disagreed_case.reshape(1,-1))[0]\n",
    "    \n",
    "    exp1_result = classes[index1]\n",
    "    exp2_result = classes[index2]\n",
    "    \n",
    "    return exp1_map[1], exp2_map[1], disagreed_case, exp1_result, exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a21e5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagreed_LIME_multi(train1, train2, all_disagreed_testcases, model1, model2, features, classes):\n",
    "    train1_numpy = train1.to_numpy()\n",
    "    train2_numpy = train2.to_numpy()\n",
    "    all_disagreed_testcases_numpy = all_disagreed_testcases[features].to_numpy()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train1_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    i = np.random.randint(0, len(all_disagreed_testcases_numpy))\n",
    "    disagreed_case = all_disagreed_testcases_numpy[i]\n",
    "    \n",
    "    index1= int(model1.predict(disagreed_case.reshape(1,-1))[0]) - min(classes)\n",
    "    index2= int(model2.predict(disagreed_case.reshape(1,-1))[0]) - min(classes)\n",
    "\n",
    "    exp1 = explainer.explain_instance(disagreed_case, model1.predict_proba, num_features=len(features), labels=[index1])\n",
    "    exp1_map = exp1.as_map()\n",
    "    exp1.show_in_notebook()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train2_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    exp2 = explainer.explain_instance(disagreed_case, model2.predict_proba, num_features=len(features), labels=[index2])\n",
    "    exp2_map = exp2.as_map()\n",
    "    exp2.show_in_notebook()\n",
    "    \n",
    "    exp1_result = classes[index1]\n",
    "    exp2_result = classes[index2]\n",
    "    \n",
    "    return exp1_map[index1], exp2_map[index2], disagreed_case, exp1_result, exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20f467c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_LIME(train_set, test_set, model, features, class_name):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train_set.to_numpy(),\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=class_name,\n",
    "                                                       verbose=True,\n",
    "                                                       mode='regression')\n",
    "    test_set_numpy = test_set.to_numpy()\n",
    "    i = np.random.randint(0, len(test_set_numpy))\n",
    "    exp = explainer.explain_instance(test_set_numpy[i], model.predict, num_features=len(features))\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3715a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIME_compare_bar_plot(features, exp1_map, exp2_map, exp1_result, exp2_result):\n",
    "    exp1_map.sort()\n",
    "    exp2_map.sort()\n",
    "    \n",
    "    exp1_map_df = pd.DataFrame(exp1_map)\n",
    "    exp2_map_df = pd.DataFrame(exp2_map)\n",
    "    \n",
    "    exp1_map_df[0] = features\n",
    "    exp2_map_df[0] = features\n",
    "    \n",
    "    merged_df= pd.merge(exp1_map_df, exp2_map_df, on=0)\n",
    "    \n",
    "    ax = merged_df.plot.barh(figsize=(9,7))\n",
    "    plt.yticks(np.arange(len(features)),features)\n",
    "    plt.legend([exp1_result, exp2_result])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc378f9a",
   "metadata": {},
   "source": [
    "labels = ['Younger Diabetes', 'Younger No Diabetes', 'Diabetes', 'No Diabetes', 'Older Diabetes', 'Older No Diabetes']\n",
    "\n",
    "sources = [0, 0, 1, 1, 4, 4, 5, 5] \n",
    "\n",
    "targets = [2, 3, 2, 3, 2, 3, 2, 3]\n",
    "\n",
    "- #num(predicted class1 by v1 that are of class1), \n",
    "- #num(predicted class1 by v1 that are of class2), â€¦,\n",
    "\n",
    "values = [younger_y_pred_confusion[1][1], younger_y_pred_confusion[1][0],\n",
    "          younger_y_pred_confusion[0][1], younger_y_pred_confusion[0][0],\n",
    "          older_y_pred_opposite_confusion[1][1], older_y_pred_opposite_confusion[1][0],\n",
    "          older_y_pred_opposite_confusion[0][1], older_y_pred_opposite_confusion[0][0],]\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = sources,\n",
    "      target = targets,\n",
    "      value = values\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfb89909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sankey_diagram(title, classes_labels, confusion_matrix1, confusion_matrix2):\n",
    "    number_of_classes = len(classes_labels)\n",
    "\n",
    "    labels = classes_labels * 3\n",
    "    \n",
    "    targets = list()\n",
    "    for i in range(number_of_classes * 2 - 1, number_of_classes * 2 - number_of_classes - 1, -1):\n",
    "        for j in range(1, 1 + number_of_classes):\n",
    "            targets.append(i)\n",
    "    start = targets.copy()\n",
    "    for i in range(number_of_classes):\n",
    "        for j in range(3 * number_of_classes - 1, 2 * number_of_classes -1, -1):\n",
    "            targets.append(j)\n",
    "\n",
    "    sources = list()\n",
    "    for i in range(number_of_classes):\n",
    "        for j in range(number_of_classes - 1, -1, -1):\n",
    "            sources.append(j)\n",
    "\n",
    "    sources += start\n",
    "    \n",
    "    values = list(confusion_matrix1.ravel()) + list(confusion_matrix2.ravel())\n",
    "    \n",
    "    my_colors = [('rgba('+str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))) for i in range(len(classes_labels))]\n",
    "    my_colors_node = []\n",
    "    my_colors_opac = []\n",
    "\n",
    "    for rgba in my_colors:\n",
    "        my_colors_node.append(rgba + ',0.8)')\n",
    "        my_colors_opac.append(rgba + ',0.4)')\n",
    "    my_colors_opac = my_colors_opac * number_of_classes\n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),    \n",
    "        label = labels,\n",
    "        color = my_colors_node * 3\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = sources,\n",
    "        target = targets,\n",
    "        value = values,\n",
    "        color = my_colors_opac[::-1] + my_colors_opac[::-1]\n",
    "    ))])\n",
    "\n",
    "    fig.update_layout(title_text=title, font_size=12)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
