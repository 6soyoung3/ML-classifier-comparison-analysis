{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f7a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import lime\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e600f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(df, sep = \",\"):\n",
    "    return pd.read_csv(df, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a01c1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(df, filepath):\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f7ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_columns(df):\n",
    "    return df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e209573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_tree(criterion=\"gini\", splitter=\"best\"):\n",
    "    model = DecisionTreeClassifier(criterion=criterion, splitter=splitter, random_state=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8f3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decision_tree(train_X, train_y, model):\n",
    "    model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e42b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(testcases, prediction_result):\n",
    "    return round(accuracy_score(testcases, prediction_result) * 100.00, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1237ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree_agreed_disagreed(all_cases, features):\n",
    "    train_X = all_cases[features]\n",
    "    train_y = all_cases.Agreed\n",
    "    model = create_decision_tree()\n",
    "    fit_decision_tree(train_X, train_y, model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4aa22f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ML_model(file_name, model):\n",
    "    filehandler = open(file_name + '.obj', 'wb') \n",
    "    pickle.dump(model, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab96f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, testcases):\n",
    "    return model.predict(testcases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c74c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ML_model(file_to_open):\n",
    "    filehandler = open(file_to_open, 'rb') \n",
    "    return pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca5dcac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(prediction_result1, prediction_result2):\n",
    "    return round(accuracy_score(prediction_result1, prediction_result2) * 100.00, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e4baace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_disagreed_cases(testcases, similarity):\n",
    "    wrong_proportion = 1 - (similarity / 100)\n",
    "    return round(len(testcases) * wrong_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8515b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_agreed_cases(testcases, prediction_result1, prediction_result2):\n",
    "    filters = []\n",
    "    for i in range(len(testcases)):\n",
    "        filters.append(prediction_result1[i] == prediction_result2[i])\n",
    "    disagreed = testcases[filters]\n",
    "    disagreed.loc[:,'Agreed'] = 1\n",
    "    return disagreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0530569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_disagreed_cases(testcases, prediction_result1, prediction_result2):\n",
    "    filters = return_agreed_cases(testcases,prediction_result1, prediction_result2)\n",
    "    agreed = testcases.loc[testcases.index.difference(filters.index)]\n",
    "    agreed.loc[:,'Agreed'] = 0\n",
    "    return agreed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3c9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     return feature_importance_output.plot.barh()\n",
    "def print_feature_importance(fittedmodel, features):\n",
    "    feature_importance = fittedmodel.feature_importances_\n",
    "    feature_importance_output = pd.DataFrame(feature_importance, features)\n",
    "    feature_importance_output.set_axis(['Output'], axis=1, inplace=True)\n",
    "    print(feature_importance_output)\n",
    "    ax = feature_importance_output.plot.barh(figsize=(9,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42e8a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importance_regression(fittedmodel, features):\n",
    "    importance = fittedmodel.coef_\n",
    "    importance = importance.reshape(-1,1)\n",
    "    df = pd.DataFrame(importance, features)\n",
    "    df.set_axis(['Feature coefficient'], axis=1, inplace=True)\n",
    "    ax = df.plot.barh(figsize=(9,5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78422e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_for_disagreement(features, agreed_cases, disagreed_cases):\n",
    "    all_cases = pd.DataFrame(agreed_cases.append(disagreed_cases))\n",
    "    X_train = all_cases[features]\n",
    "    y_train = all_cases['Agreed']\n",
    "    model = create_decision_tree()\n",
    "    fit_decision_tree(X_train, y_train, model)\n",
    "    print_feature_importance(model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c12cb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_comparison(model1, model2, features, name1, name2):\n",
    "    feature_importance1 = model1.feature_importances_\n",
    "    feature_importance2 = model2.feature_importances_\n",
    "    feature_importance1_output = pd.DataFrame(feature_importance1, features)\n",
    "    feature_importance2_output = pd.DataFrame(feature_importance2, features)\n",
    "    concat_df = pd.concat([feature_importance1_output, feature_importance2_output],axis=1)\n",
    "    concat_df.set_axis([name1,name2], axis=1, inplace=True)\n",
    "    ax=concat_df.plot.barh(figsize=(9,7))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "015de23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_comparison_regression(model1, model2, features, name1, name2):\n",
    "    feature_importance1 = (model1.coef_).reshape(-1,1)\n",
    "    feature_importance2 = (model2.coef_).reshape(-1,1)\n",
    "    feature_importance1_output = pd.DataFrame(feature_importance1, features)\n",
    "    feature_importance2_output = pd.DataFrame(feature_importance2, features)\n",
    "    concat_df = pd.concat([feature_importance1_output, feature_importance2_output],axis=1)\n",
    "    concat_df.set_axis([name1,name2], axis=1, inplace=True)\n",
    "    ax=concat_df.plot.barh(figsize=(9,7))\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7d29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagreed_LIME(train1, train2, all_disagreed_testcases, model1, model2, features, classes):\n",
    "    train1_numpy = train1.to_numpy()\n",
    "    train2_numpy = train2.to_numpy()\n",
    "    all_disagreed_testcases_numpy = all_disagreed_testcases[features].to_numpy()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train1_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    i = np.random.randint(0, len(all_disagreed_testcases_numpy))\n",
    "    disagreed_case = all_disagreed_testcases_numpy[i]\n",
    "\n",
    "    exp1 = explainer.explain_instance(disagreed_case, model1.predict_proba, num_features=len(features))\n",
    "    exp1_map = exp1.as_map()\n",
    "    exp1.show_in_notebook()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train2_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    exp2 = explainer.explain_instance(disagreed_case, model2.predict_proba, num_features=len(features))\n",
    "    exp2_map = exp2.as_map()\n",
    "    exp2.show_in_notebook()\n",
    "    \n",
    "    index1= model1.predict(disagreed_case.reshape(1,-1))[0]\n",
    "    index2= model2.predict(disagreed_case.reshape(1,-1))[0]\n",
    "    \n",
    "    exp1_result = classes[index1]\n",
    "    exp2_result = classes[index2]\n",
    "    \n",
    "    return exp1_map[1], exp2_map[1], disagreed_case, exp1_result, exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a21e5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disagreed_LIME_multi(train1, train2, all_disagreed_testcases, model1, model2, features, classes):\n",
    "    train1_numpy = train1.to_numpy()\n",
    "    train2_numpy = train2.to_numpy()\n",
    "    all_disagreed_testcases_numpy = all_disagreed_testcases[features].to_numpy()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train1_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    i = np.random.randint(0, len(all_disagreed_testcases_numpy))\n",
    "    disagreed_case = all_disagreed_testcases_numpy[i]\n",
    "    \n",
    "    index1= int(model1.predict(disagreed_case.reshape(1,-1))[0]) - min(classes)\n",
    "    index2= int(model2.predict(disagreed_case.reshape(1,-1))[0]) - min(classes)\n",
    "\n",
    "    exp1 = explainer.explain_instance(disagreed_case, model1.predict_proba, num_features=len(features), labels=[index1])\n",
    "    exp1_map = exp1.as_map()\n",
    "    exp1.show_in_notebook()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train2_numpy,\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=classes,\n",
    "                                                       discretize_continuous=True)\n",
    "\n",
    "    exp2 = explainer.explain_instance(disagreed_case, model2.predict_proba, num_features=len(features), labels=[index2])\n",
    "    exp2_map = exp2.as_map()\n",
    "    exp2.show_in_notebook()\n",
    "    \n",
    "    exp1_result = classes[index1]\n",
    "    exp2_result = classes[index2]\n",
    "    \n",
    "    return exp1_map[index1], exp2_map[index2], disagreed_case, exp1_result, exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20f467c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_LIME(train_set, test_set, model, features, class_name):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(train_set.to_numpy(),\n",
    "                                                       feature_names=features,\n",
    "                                                       class_names=class_name,\n",
    "                                                       verbose=True,\n",
    "                                                       mode='regression')\n",
    "    test_set_numpy = test_set.to_numpy()\n",
    "    i = np.random.randint(0, len(test_set_numpy))\n",
    "    exp = explainer.explain_instance(test_set_numpy[i], model.predict, num_features=len(features))\n",
    "    exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3715a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIME_compare_bar_plot(features, exp1_map, exp2_map, exp1_result, exp2_result):\n",
    "    exp1_map.sort()\n",
    "    exp2_map.sort()\n",
    "    \n",
    "    exp1_map_df = pd.DataFrame(exp1_map)\n",
    "    exp2_map_df = pd.DataFrame(exp2_map)\n",
    "    \n",
    "    exp1_map_df[0] = features\n",
    "    exp2_map_df[0] = features\n",
    "    \n",
    "    merged_df= pd.merge(exp1_map_df, exp2_map_df, on=0)\n",
    "    \n",
    "    ax = merged_df.plot.barh(figsize=(9,7))\n",
    "    plt.yticks(np.arange(len(features)),features)\n",
    "    plt.legend([exp1_result, exp2_result])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc378f9a",
   "metadata": {},
   "source": [
    "labels = ['Younger Diabetes', 'Younger No Diabetes', 'Diabetes', 'No Diabetes', 'Older Diabetes', 'Older No Diabetes']\n",
    "\n",
    "sources = [0, 0, 1, 1, 4, 4, 5, 5] \n",
    "\n",
    "targets = [2, 3, 2, 3, 2, 3, 2, 3]\n",
    "\n",
    "- #num(predicted class1 by v1 that are of class1), \n",
    "- #num(predicted class1 by v1 that are of class2), …,\n",
    "\n",
    "values = [younger_y_pred_confusion[1][1], younger_y_pred_confusion[1][0],\n",
    "          younger_y_pred_confusion[0][1], younger_y_pred_confusion[0][0],\n",
    "          older_y_pred_opposite_confusion[1][1], older_y_pred_opposite_confusion[1][0],\n",
    "          older_y_pred_opposite_confusion[0][1], older_y_pred_opposite_confusion[0][0],]\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = sources,\n",
    "      target = targets,\n",
    "      value = values\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfb89909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sankey_diagram(title, classes, pred1, pred2, ground_truth):\n",
    "    number_of_classes = len(classes)\n",
    "\n",
    "    classes_labels = list()\n",
    "    for i in range(max(classes), min(classes) - 1, -1):\n",
    "        classes_labels.append(\"Class \" + str(i))\n",
    "    \n",
    "    labels = classes_labels * 3\n",
    "    \n",
    "    confusion_matrix1 = confusion_matrix(ground_truth, pred1, labels=classes)\n",
    "    confusion_matrix2 = confusion_matrix(ground_truth, pred2, labels=classes)\n",
    "    \n",
    "    targets = list()\n",
    "    for i in range(number_of_classes * 2 - 1, number_of_classes * 2 - number_of_classes - 1, -1):\n",
    "        for j in range(1, 1 + number_of_classes):\n",
    "            targets.append(i)\n",
    "    start = targets.copy()\n",
    "    for i in range(number_of_classes):\n",
    "        for j in range(3 * number_of_classes - 1, 2 * number_of_classes -1, -1):\n",
    "            targets.append(j)\n",
    "\n",
    "    sources = list()\n",
    "    for i in range(number_of_classes):\n",
    "        for j in range(number_of_classes - 1, -1, -1):\n",
    "            sources.append(j)\n",
    "\n",
    "    sources += start\n",
    "    \n",
    "    values = list(confusion_matrix1.ravel()) + list(confusion_matrix2.ravel())\n",
    "    \n",
    "    my_colors = [('rgba('+str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))+','+\n",
    "                str(np.random.randint(0, high = 256))) for i in range(len(classes_labels))]\n",
    "    my_colors_node = []\n",
    "    my_colors_opac = []\n",
    "\n",
    "    for rgba in my_colors:\n",
    "        my_colors_node.append(rgba + ',0.8)')\n",
    "        my_colors_opac.append(rgba + ',0.4)')\n",
    "    my_colors_opac = my_colors_opac * number_of_classes\n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),    \n",
    "        label = labels,\n",
    "        color = my_colors_node * 3\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = sources,\n",
    "        target = targets,\n",
    "        value = values,\n",
    "        color = my_colors_opac[::-1] + my_colors_opac[::-1]\n",
    "    ))])\n",
    "\n",
    "    fig.update_layout(title_text=title, font_size=12)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f8f1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LinearExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.LinearExplainer(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26b2e532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_KernelExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.KernelExplainer(model.predict_proba, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cf39c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_TreeExplainer(model, train_data, sample = None):\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "    return shap.TreeExplainer(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f7e4045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_SHAP_info(shap_explainer, model, test_data, index):\n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "\n",
    "    print(\"Base Value : \", shap_explainer.expected_value)\n",
    "    print()\n",
    "    print(\"Shap Values for Sample %d: \" %(index), shap_values[index])\n",
    "    print(\"\\n\")\n",
    "    print(\"Prediction From Model                            : \", model.predict((test_data.iloc[index]).to_numpy().reshape(1,-1))[0])\n",
    "    print(\"Prediction From Adding SHAP Values to Base Value : \", shap_explainer.expected_value + shap_values.sum())\n",
    "    print(\"Input instance:                                  : \", test_data.iloc[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "189da472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_regression_force_plot(shap_explainer, test_data):\n",
    "    shap.initjs()\n",
    "\n",
    "    base_value = shap_explainer.expected_value\n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "\n",
    "    return shap.force_plot(base_value, shap_values, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d286bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_classification_force_plot(shap_explainer, test_data, class_index, sample=None):\n",
    "    shap.initjs()\n",
    "    \n",
    "    if sample != None:\n",
    "        test_data = shap.sample(test_data, sample)\n",
    "    \n",
    "    base_value = shap_explainer.expected_value\n",
    "    shap_values = shap_explainer.shap_values(test_data)\n",
    "    \n",
    "    return shap.force_plot(base_value[class_index], shap_values[class_index], test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53f31baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_waterfall_plot(shap_explainer, test_data, sample_index):\n",
    "    shap.initjs()\n",
    "    shap_values = shap_explainer(test_data)\n",
    "    return shap.plots.waterfall(shap_values[sample_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc91a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_waterfall_plot(shap_explainer, data, class_index, sample_index):\n",
    "    shap.initjs()\n",
    "    \n",
    "#     data = diabetes_all_testcases[diabetes_features]\n",
    "#     explainer = shap.TreeExplainer(diabetes_all_testcases_decisionTree_model, data)\n",
    "    \n",
    "    shap_values = shap_explainer.shap_values(data)\n",
    "    return shap.waterfall_plot(shap.Explanation(values=shap_values[class_index][sample_index], \n",
    "                                     base_values=shap_explainer.expected_value[class_index],\n",
    "                                     data=data.iloc[sample_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53c35fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_feature_importance(shap_explainer, train_data, features, classes, sample):\n",
    "    shap.initjs()\n",
    "    if sample != None:\n",
    "        train_data = shap.sample(train_data, sample)\n",
    "#     data = shap.sample(less_X_train, 10)\n",
    "#     shap_explainer = shap.KernelExplainer(less_knn_model.predict_proba, data)\n",
    "    shap_values = shap_explainer.shap_values(train_data)\n",
    "    return shap.summary_plot(shap_values, train_data, feature_names=features, plot_type=\"bar\", class_names=classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
